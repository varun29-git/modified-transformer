{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOPaYpxI73qBkElQIVYaNY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varun29-git/modified-transformer/blob/main/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqZi6qRFiAY0",
        "outputId": "7e0a81f5-594d-4edd-bd71-12b89b828eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modified-transformer\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/varun29-git/modified-transformer -q\n",
        "%cd modified-transformer/\n",
        "!pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH30BQZliJv9",
        "outputId": "7b17b03b-6615-4f5e-fdd1-24d00e3b8f9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/VectorSLM')"
      ],
      "metadata": {
        "id": "llKNOJS5iK-4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "from model import build_transformer\n",
        "from config import *\n"
      ],
      "metadata": {
        "id": "Qai2YcAsiPCb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok4VbFfuiQyr",
        "outputId": "fba9fcc1-2a8a-456a-bc70-8f84fdb59c53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "vocab_size = tokenizer.n_vocab\n"
      ],
      "metadata": {
        "id": "YlWKLK6FiSUk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading model...\")\n",
        "\n",
        "model = build_transformer(\n",
        "    vocab_size,\n",
        "    D_MODEL,\n",
        "    H,\n",
        "    N,\n",
        "    D_FF,\n",
        "    DROPOUT\n",
        ").to(device)\n",
        "\n",
        "weights_path = \"/content/drive/MyDrive/modified_transformer_weights/_weights/best_model.pt\"\n",
        "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKHlX-X5iUn1",
        "outputId": "a54a7350-8580-4de8-bad6-6ffa77f9a0b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(\n",
        "    prompt,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    max_new_tokens=70,\n",
        "    temperature=0.8\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        seq_len = tokens.size(1)\n",
        "\n",
        "        # causal mask\n",
        "        mask = torch.tril(\n",
        "            torch.ones(seq_len, seq_len, device=device)\n",
        "        ).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        logits = model(tokens, mask)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.eot_token:\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(tokens[0].tolist())\n"
      ],
      "metadata": {
        "id": "xFqrwY3-iXTh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"He was unhappy\",\n",
        "    \"The brave knight went to the\",\n",
        "    \"In a magical forest, a small\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"\\n{'='*20} STORY {i} {'='*20}\")\n",
        "    print(generate_text(prompt, model, tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmvM8foVialr",
        "outputId": "12c0dd48-a6bf-4e33-f58f-6623bbdd4fd8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== STORY 1 ====================\n",
            "He was unhappy and said, \"What do you mean? This is not nice.\"\n",
            "\n",
            "The girl said, \"I just wanted to do it, but I did not know that. I just wanted some milk too. Now I can make two pies for you. And you can have some tea and a snack. And you can play with your dolls instead.\"\n",
            "\n",
            "The girl\n",
            "\n",
            "==================== STORY 2 ====================\n",
            "The brave knight went to the beach. It was a picture of a castle and even a castle. The knight was so happy, he decided to close the picture of himself. \n",
            "\n",
            "The knight learned a valuable lesson that day - when you show me what you used, you can make something truly special.Once upon a time there was a little girl named Gus. lying in the forest\n",
            "\n",
            "==================== STORY 3 ====================\n",
            "In a magical forest, a small rabbit and a rabbit were playing in the water. The rabbit was very excited and wanted to stay for a long time.\n",
            "\n",
            "The rabbit asked the rabbit why the rabbit was so flexible. The rabbit told the rabbit he could move around by himself in circles.\n",
            "The rabbit said he must surrender one more time and make a wish. The rabbit hopped off and\n"
          ]
        }
      ]
    }
  ]
}